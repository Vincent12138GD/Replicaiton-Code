{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id  LUSS filetype     source   probe  class     type\n",
      "0  1_butterfly_covid   3.0      mp4  Butterfly  Convex  COVID     lung\n",
      "1  2_butterfly_covid   2.0      mp4  Butterfly  Convex  COVID     lung\n",
      "2  3_butterfly_covid   2.0      mp4  Butterfly  Convex  COVID     lung\n",
      "3  4_butterfly_covid   2.0      mp4  Butterfly  Convex  COVID     lung\n",
      "4  5_butterfly_covid   NaN      mp4  Butterfly  Convex  COVID  cardiac\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the relative path to the CSV file\n",
    "metadata_path = '../COVID-US/utils/LUSS_metadata.csv'\n",
    "\n",
    "# Read the CSV file using the relative path\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Display the data \n",
    "print(metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    video_number = int(parts[0])\n",
    "    return video_number\n",
    "\n",
    "# Example usage\n",
    "filename = \"1_butterfly_covid_prc_convex_clean_frame0.jpg\"\n",
    "video_number = parse_filename(filename)\n",
    "print(video_number)  # Output: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZED_IMAGE= '../COVID-US/data/image/clean/resized'\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Example usage\n",
    "resized_dir = '../COVID-US/data/image/clean/resized'\n",
    "ensure_directory_exists(RESIZED_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_image(image_path, resized_dir, target_size=(224, 224), normalization_range=(0, 1)):\n",
    "    filename = os.path.basename(image_path)\n",
    "    resized_filename = filename.replace('.jpg', '_resized.jpg')\n",
    "    resized_image_path = os.path.join(resized_dir, resized_filename)\n",
    "    \n",
    "    # Check if the resized image already exists\n",
    "    if os.path.exists(resized_image_path):\n",
    "        image = cv2.imread(resized_image_path)\n",
    "        if image is not None:\n",
    "            return image\n",
    "    \n",
    "    # If the resized image does not exist, create it\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        image = cv2.resize(image, target_size)\n",
    "        cv2.imwrite(resized_image_path, image)  # Save resized image\n",
    "        \n",
    "        # Normalize image\n",
    "        image = image.astype('float32')\n",
    "        if normalization_range == (0, 1):\n",
    "            image /= 255.0\n",
    "        elif normalization_range == (-1, 1):\n",
    "            image = (image / 127.5) - 1.0\n",
    "        return image\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(image_dir, resized_dir, metadata, target_size=(224, 224), normalization_range=(0, 1)):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "    probe_types = []\n",
    "    image_paths = []\n",
    "    \n",
    "    ensure_directory_exists(resized_dir)\n",
    "    \n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            video_number = parse_filename(filename)\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            image = preprocess_and_save_image(img_path, resized_dir, target_size, normalization_range)\n",
    "            if image is not None:\n",
    "                image_data.append(image)\n",
    "                image_paths.append(img_path)\n",
    "                \n",
    "                # Extract corresponding metadata\n",
    "                video_metadata = metadata[metadata['id'].str.startswith(str(video_number) + '_')]\n",
    "                if not video_metadata.empty:\n",
    "                    labels.append(video_metadata['class'].values[0])\n",
    "                    probe_types.append(video_metadata['probe'].values[0])\n",
    "                else:\n",
    "                    print(f\"No metadata found for video number {video_number}\")\n",
    "\n",
    "    return np.array(image_data), labels, probe_types, image_paths\n",
    "\n",
    "CLEAN_IMAGE = '../COVID-US/data/image/clean'\n",
    "RESIZED_IMAGE = '../COVID-US/data/image/clean/resized'\n",
    "images, labels, probe_types, image_paths = load_and_preprocess_images(CLEAN_IMAGE, RESIZED_IMAGE, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(probe_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21263, 224, 224, 3)\n",
      "probe_train shape: (21263, 2)\n",
      "X_val shape: (4866, 224, 224, 3)\n",
      "probe_val shape: (4866, 2)\n",
      "X_test shape: (3516, 224, 224, 3)\n",
      "probe_test shape: (3516, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract video IDs from image paths (assuming filenames are formatted as \"videoID_frameID_resized.jpg\")\n",
    "video_ids = [os.path.basename(path).split('_')[0] for path in image_paths]\n",
    "\n",
    "# Convert to NumPy array\n",
    "video_ids = np.array(video_ids)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# One-hot encode probe types\n",
    "probe_type_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_probe_types = probe_type_encoder.fit_transform(np.array(probe_types).reshape(-1, 1))\n",
    "\n",
    "# Create a DataFrame with minimal information for splitting\n",
    "data = pd.DataFrame({\n",
    "    'video_id': video_ids,\n",
    "    'label': encoded_labels\n",
    "})\n",
    "\n",
    "# Map video IDs to their corresponding labels\n",
    "video_to_label = data.groupby('video_id')['label'].apply(lambda x: x.mode()[0]).to_dict()\n",
    "\n",
    "# Get the label for each unique video ID\n",
    "unique_video_ids = data['video_id'].unique()\n",
    "video_labels = np.array([video_to_label[vid] for vid in unique_video_ids])\n",
    "\n",
    "# Split video IDs into training, validation, and test sets\n",
    "train_videos, test_videos = train_test_split(unique_video_ids, test_size=0.1, random_state=42, stratify=video_labels)\n",
    "train_labels = np.array([video_to_label[vid] for vid in train_videos])\n",
    "train_videos, val_videos = train_test_split(train_videos, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "# Assign frames to the respective sets based on their video split\n",
    "train_indices = data[data['video_id'].isin(train_videos)].index\n",
    "val_indices = data[data['video_id'].isin(val_videos)].index\n",
    "test_indices = data[data['video_id'].isin(test_videos)].index\n",
    "\n",
    "# Use indices to extract the actual data\n",
    "X_train = np.stack([images[i] for i in train_indices])\n",
    "y_train = encoded_labels[train_indices]\n",
    "probe_train = np.stack([encoded_probe_types[i] for i in train_indices])\n",
    "\n",
    "X_val = np.stack([images[i] for i in val_indices])\n",
    "y_val = encoded_labels[val_indices]\n",
    "probe_val = np.stack([encoded_probe_types[i] for i in val_indices])\n",
    "\n",
    "X_test = np.stack([images[i] for i in test_indices])\n",
    "y_test = encoded_labels[test_indices]\n",
    "probe_test = np.stack([encoded_probe_types[i] for i in test_indices])\n",
    "\n",
    "# Validate input shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"probe_train shape:\", probe_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"probe_val shape:\", probe_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"probe_test shape:\", probe_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Train  Validation  Test\n",
      "Class Name                         \n",
      "COVID        6376        1875   970\n",
      "Normal       2093         579   209\n",
      "Other        7844        1115  1206\n",
      "Pneumonia    4950        1297  1131\n"
     ]
    }
   ],
   "source": [
    "# Record the distribution of labels in each set\n",
    "train_label_distribution = pd.Series(y_train).value_counts().sort_index()\n",
    "val_label_distribution = pd.Series(y_val).value_counts().sort_index()\n",
    "test_label_distribution = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "# Map encoded labels back to original class names\n",
    "label_class_names = label_encoder.inverse_transform(train_label_distribution.index)\n",
    "\n",
    "# Create a DataFrame to store the distributions with class names\n",
    "label_distributions = pd.DataFrame({\n",
    "    'Class Name': label_class_names,\n",
    "    'Train': train_label_distribution.values,\n",
    "    'Validation': val_label_distribution.values,\n",
    "    'Test': test_label_distribution.values\n",
    "}).set_index('Class Name')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(label_distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the label distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "label_distributions.plot(kind='bar')\n",
    "plt.title('Label Distribution')\n",
    "plt.xlabel('Class Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2, InceptionV3, InceptionResNetV2, Xception, ResNet50V2,\n",
    "    EfficientNetB4, DenseNet121, DenseNet169, DenseNet201\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "def build_baseline_model(base_model, input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = base_model(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    cnn_input = Input(shape=input_shape, name='cnn_input')\n",
    "    cnn_output = base_model(cnn_input)\n",
    "    cnn_output = Flatten()(cnn_output)\n",
    "    final_output = Dense(num_classes, activation='softmax')(cnn_output)\n",
    "\n",
    "    model = Model(inputs=cnn_input, outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_multimodal_model(base_model, input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = base_model(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    cnn_input = Input(shape=input_shape, name='cnn_input')\n",
    "    cnn_output = base_model(cnn_input)\n",
    "    cnn_output = Flatten()(cnn_output)\n",
    "\n",
    "    probe_input_shape = (encoded_probe_types.shape[1],)\n",
    "    probe_input = Input(shape=probe_input_shape, name='probe_input')\n",
    "    x = Dense(1024, activation='relu')(probe_input)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "\n",
    "    combined = Concatenate()([cnn_output, x])\n",
    "    final_output = Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, probe_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, probe_train, X_val, y_val, probe_val, epochs=20, batch_size=16, multimodal=True):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    if multimodal:\n",
    "        history = model.fit(\n",
    "            [X_train, probe_train], y_train,\n",
    "            validation_data=([X_val, probe_val], y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, probe_test, multimodal=True):\n",
    "    if multimodal:\n",
    "        y_pred = model.predict([X_test, probe_test])\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    return accuracy, conf_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21263, 224, 224, 3)\n",
      "probe_train shape: (21263, 2)\n",
      "X_val shape: (4866, 224, 224, 3)\n",
      "probe_val shape: (4866, 2)\n",
      "X_test shape: (3516, 224, 224, 3)\n",
      "probe_test shape: (3516, 2)\n"
     ]
    }
   ],
   "source": [
    "# Validate input shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"probe_train shape:\", probe_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"probe_val shape:\", probe_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"probe_test shape:\", probe_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Modify `run_model_evaluation` to return metrics\n",
    "# def run_model_evaluation(base_model_name, base_model, X_train, y_train, probe_train, X_val, y_val, probe_val, X_test, y_test, probe_test):\n",
    "#     metrics = {\n",
    "#         'model_name': base_model_name,\n",
    "#         'baseline': {},\n",
    "#         'multimodal': {}\n",
    "#     }\n",
    "    \n",
    "#     print(f'Evaluating {base_model_name} Baseline Model')\n",
    "#     baseline_model = build_baseline_model(base_model)\n",
    "#     baseline_history = train_model(baseline_model, X_train, y_train, None, X_val, y_val, None, multimodal=False)\n",
    "#     baseline_accuracy, baseline_conf_matrix = evaluate_model(baseline_model, X_test, y_test, None, multimodal=False)\n",
    "    \n",
    "#     metrics['baseline']['history'] = baseline_history.history\n",
    "#     metrics['baseline']['accuracy'] = baseline_accuracy\n",
    "#     metrics['baseline']['conf_matrix'] = baseline_conf_matrix\n",
    "    \n",
    "#     print(f'Evaluating {base_model_name} Multimodal Model')\n",
    "#     multimodal_model = build_multimodal_model(base_model)\n",
    "#     multimodal_history = train_model(multimodal_model, X_train, y_train, probe_train, X_val, y_val, probe_val, multimodal=True)\n",
    "#     multimodal_accuracy, multimodal_conf_matrix = evaluate_model(multimodal_model, X_test, y_test, probe_test, multimodal=True)\n",
    "    \n",
    "#     metrics['multimodal']['history'] = multimodal_history.history\n",
    "#     metrics['multimodal']['accuracy'] = multimodal_accuracy\n",
    "#     metrics['multimodal']['conf_matrix'] = multimodal_conf_matrix\n",
    "    \n",
    "#     return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_evaluation(base_model_name, base_model, X_train, y_train, probe_train, X_val, y_val, probe_val, X_test, y_test, probe_test, results, epochs=20):\n",
    "    metrics = {\n",
    "        'model_name': base_model_name,\n",
    "        'baseline': {},\n",
    "        'multimodal': {}\n",
    "    }\n",
    "    \n",
    "    # Define paths for model weights\n",
    "    baseline_weights_path = f'./model_weights/{base_model_name}_baseline.weights.h5'\n",
    "    multimodal_weights_path = f'./model_weights/{base_model_name}_multimodal.weights.h5'\n",
    "    \n",
    "    # Check if the model results already exist in the results list\n",
    "    existing_model_metrics = next((item for item in results if item['model_name'] == base_model_name), None)\n",
    "    \n",
    "    retrain_baseline = True\n",
    "    retrain_multimodal = True\n",
    "\n",
    "    # Check conditions for skipping baseline training\n",
    "    if existing_model_metrics:\n",
    "        baseline_history_len = len(existing_model_metrics['baseline'].get('history', {}).get('accuracy', []))\n",
    "        if os.path.exists(baseline_weights_path) and baseline_history_len == epochs:\n",
    "            retrain_baseline = False\n",
    "\n",
    "    # Check conditions for skipping multimodal training\n",
    "    if existing_model_metrics:\n",
    "        multimodal_history_len = len(existing_model_metrics['multimodal'].get('history', {}).get('accuracy', []))\n",
    "        if os.path.exists(multimodal_weights_path) and multimodal_history_len == epochs:\n",
    "            retrain_multimodal = False\n",
    "\n",
    "    # Evaluate baseline model\n",
    "    print(f'Evaluating {base_model_name} Baseline Model')\n",
    "    baseline_model = build_baseline_model(base_model)\n",
    "    \n",
    "    if retrain_baseline:\n",
    "        baseline_history = train_model(baseline_model, X_train, y_train, None, X_val, y_val, None, epochs=epochs, multimodal=False)\n",
    "        baseline_model.save_weights(baseline_weights_path)\n",
    "        metrics['baseline']['history'] = baseline_history.history\n",
    "    else:\n",
    "        print(f\"Loading existing weights for {base_model_name} baseline model.\")\n",
    "        baseline_model.load_weights(baseline_weights_path)\n",
    "        metrics['baseline'] = existing_model_metrics['baseline']\n",
    "\n",
    "    baseline_accuracy, baseline_conf_matrix = evaluate_model(baseline_model, X_test, y_test, None, multimodal=False)\n",
    "    metrics['baseline']['accuracy'] = baseline_accuracy\n",
    "    metrics['baseline']['conf_matrix'] = baseline_conf_matrix\n",
    "    \n",
    "    # Evaluate multimodal model\n",
    "    print(f'Evaluating {base_model_name} Multimodal Model')\n",
    "    multimodal_model = build_multimodal_model(base_model)\n",
    "    \n",
    "    if retrain_multimodal:\n",
    "        multimodal_history = train_model(multimodal_model, X_train, y_train, probe_train, X_val, y_val, probe_val, epochs=epochs, multimodal=True)\n",
    "        multimodal_model.save_weights(multimodal_weights_path)\n",
    "        metrics['multimodal']['history'] = multimodal_history.history\n",
    "    else:\n",
    "        print(f\"Loading existing weights for {base_model_name} multimodal model.\")\n",
    "        multimodal_model.load_weights(multimodal_weights_path)\n",
    "        metrics['multimodal'] = existing_model_metrics['multimodal']\n",
    "\n",
    "    multimodal_accuracy, multimodal_conf_matrix = evaluate_model(multimodal_model, X_test, y_test, probe_test, multimodal=True)\n",
    "    metrics['multimodal']['accuracy'] = multimodal_accuracy\n",
    "    metrics['multimodal']['conf_matrix'] = multimodal_conf_matrix\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MobileNetV2 Baseline Model\n",
      "Loading existing weights for MobileNetV2 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step\n",
      "Accuracy: 54.49%\n",
      "Confusion Matrix:\n",
      "[[370  28 482  90]\n",
      " [  0   0 209   0]\n",
      " [207   0 988  11]\n",
      " [ 58   1 514 558]]\n",
      "Evaluating MobileNetV2 Multimodal Model\n",
      "Loading existing weights for MobileNetV2 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 156ms/step\n",
      "Accuracy: 54.84%\n",
      "Confusion Matrix:\n",
      "[[ 370    1  524   75]\n",
      " [   0    0  209    0]\n",
      " [ 171    0 1023   12]\n",
      " [  42    0  554  535]]\n",
      "Evaluating InceptionV3 Baseline Model\n",
      "Loading existing weights for InceptionV3 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 344ms/step\n",
      "Accuracy: 50.91%\n",
      "Confusion Matrix:\n",
      "[[587   0 365  18]\n",
      " [ 55   0 154   0]\n",
      " [ 74   0 714 418]\n",
      " [440   0 202 489]]\n",
      "Evaluating InceptionV3 Multimodal Model\n",
      "Loading existing weights for InceptionV3 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 337ms/step\n",
      "Accuracy: 53.64%\n",
      "Confusion Matrix:\n",
      "[[602   0 332  36]\n",
      " [ 16   0 186   7]\n",
      " [ 81   0 736 389]\n",
      " [110   0 473 548]]\n",
      "Evaluating InceptionResNetV2 Baseline Model\n",
      "Loading existing weights for InceptionResNetV2 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 795ms/step\n",
      "Accuracy: 48.58%\n",
      "Confusion Matrix:\n",
      "[[303  81 560  26]\n",
      " [  1   0 207   1]\n",
      " [121   4 502 579]\n",
      " [106   0 122 903]]\n",
      "Evaluating InceptionResNetV2 Multimodal Model\n",
      "Loading existing weights for InceptionResNetV2 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 787ms/step\n",
      "Accuracy: 51.39%\n",
      "Confusion Matrix:\n",
      "[[304 105 517  44]\n",
      " [  4  18 187   0]\n",
      " [101   4 532 569]\n",
      " [ 92   0  86 953]]\n",
      "Evaluating Xception Baseline Model\n",
      "Loading existing weights for Xception baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 608ms/step\n",
      "Accuracy: 55.86%\n",
      "Confusion Matrix:\n",
      "[[573  13 378   6]\n",
      " [ 51   0 158   0]\n",
      " [ 65   0 842 299]\n",
      " [516   0  66 549]]\n",
      "Evaluating Xception Multimodal Model\n",
      "Loading existing weights for Xception multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 609ms/step\n",
      "Accuracy: 55.29%\n",
      "Confusion Matrix:\n",
      "[[548  42 373   7]\n",
      " [ 52   0 157   0]\n",
      " [ 77  16 809 304]\n",
      " [500   0  44 587]]\n",
      "Evaluating ResNet50V2 Baseline Model\n",
      "Loading existing weights for ResNet50V2 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 454ms/step\n",
      "Accuracy: 59.22%\n",
      "Confusion Matrix:\n",
      "[[575 197  51 147]\n",
      " [  0   0 209   0]\n",
      " [180  25 520 481]\n",
      " [ 53   0  91 987]]\n",
      "Evaluating ResNet50V2 Multimodal Model\n",
      "Loading existing weights for ResNet50V2 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 458ms/step\n",
      "Accuracy: 60.84%\n",
      "Confusion Matrix:\n",
      "[[ 547   24   90  309]\n",
      " [   0    0  209    0]\n",
      " [ 173   77  548  408]\n",
      " [  47    0   40 1044]]\n",
      "Evaluating EfficientNetB4 Baseline Model\n",
      "Loading existing weights for EfficientNetB4 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 593ms/step\n",
      "Accuracy: 58.11%\n",
      "Confusion Matrix:\n",
      "[[683  81  89 117]\n",
      " [  0   0 209   0]\n",
      " [ 39   0 689 478]\n",
      " [  1   6 453 671]]\n",
      "Evaluating EfficientNetB4 Multimodal Model\n",
      "Loading existing weights for EfficientNetB4 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 585ms/step\n",
      "Accuracy: 52.45%\n",
      "Confusion Matrix:\n",
      "[[496 243 143  88]\n",
      " [  0   5 204   0]\n",
      " [ 36   0 731 439]\n",
      " [  0   3 516 612]]\n",
      "Evaluating DenseNet121 Baseline Model\n",
      "Loading existing weights for DenseNet121 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 490ms/step\n",
      "Accuracy: 50.14%\n",
      "Confusion Matrix:\n",
      "[[351   4 278 337]\n",
      " [  0   0 209   0]\n",
      " [113   0 667 426]\n",
      " [104   0 282 745]]\n",
      "Evaluating DenseNet121 Multimodal Model\n",
      "Loading existing weights for DenseNet121 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 492ms/step\n",
      "Accuracy: 59.07%\n",
      "Confusion Matrix:\n",
      "[[387  50 278 255]\n",
      " [  0   0 209   0]\n",
      " [  0   0 792 414]\n",
      " [ 36   8 189 898]]\n",
      "Evaluating DenseNet169 Baseline Model\n",
      "Loading existing weights for DenseNet169 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 595ms/step\n",
      "Accuracy: 62.60%\n",
      "Confusion Matrix:\n",
      "[[514   0 231 225]\n",
      " [  4  21 184   0]\n",
      " [  0   1 736 469]\n",
      " [ 15  10 176 930]]\n",
      "Evaluating DenseNet169 Multimodal Model\n",
      "Loading existing weights for DenseNet169 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 590ms/step\n",
      "Accuracy: 63.05%\n",
      "Confusion Matrix:\n",
      "[[680   0 120 170]\n",
      " [  0  53 156   0]\n",
      " [155   0 569 482]\n",
      " [ 57  14 145 915]]\n",
      "Evaluating DenseNet201 Baseline Model\n",
      "Loading existing weights for DenseNet201 baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 744ms/step\n",
      "Accuracy: 51.51%\n",
      "Confusion Matrix:\n",
      "[[576   0 155 239]\n",
      " [  1  29 179   0]\n",
      " [  0  20 665 521]\n",
      " [123   2 465 541]]\n",
      "Evaluating DenseNet201 Multimodal Model\n",
      "Loading existing weights for DenseNet201 multimodal model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 759ms/step\n",
      "Accuracy: 62.43%\n",
      "Confusion Matrix:\n",
      "[[ 604    0  221  145]\n",
      " [   4   45  160    0]\n",
      " [ 163    0  537  506]\n",
      " [  69   13   40 1009]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ensure the directory for storing model weights exists\n",
    "os.makedirs('./model_weights', exist_ok=True)\n",
    "\n",
    "# Check if results.pkl exists\n",
    "if os.path.exists('results.pkl'):\n",
    "    with open('results.pkl', 'rb') as file:\n",
    "        results = pickle.load(file)\n",
    "else: results = []\n",
    "\n",
    "models = { \n",
    "    \"MobileNetV2\": MobileNetV2,\n",
    "    \"InceptionV3\": InceptionV3,\n",
    "    \"InceptionResNetV2\": InceptionResNetV2,\n",
    "    \"Xception\": Xception,\n",
    "    \"ResNet50V2\": ResNet50V2,\n",
    "    \"EfficientNetB4\": EfficientNetB4,\n",
    "    \"DenseNet121\": DenseNet121,\n",
    "    \"DenseNet169\": DenseNet169,\n",
    "    \"DenseNet201\": DenseNet201\n",
    "}\n",
    "\n",
    "# Store results in a list\n",
    "for model_name, model_class in models.items():\n",
    "    metrics = run_model_evaluation(\n",
    "        model_name, \n",
    "        model_class, \n",
    "        X_train, y_train, probe_train, \n",
    "        X_val, y_val, probe_val, \n",
    "        X_test, y_test, probe_test, \n",
    "        results\n",
    "    )\n",
    "    # Update the results list, ensuring no duplicates\n",
    "    results = [item for item in results if item['model_name'] != model_name]\n",
    "    results.append(metrics)\n",
    "\n",
    "# Save the results to a file\n",
    "with open('results.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "# Step 3: Convert results to DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "accuracy_data = []\n",
    "for result in results:\n",
    "    accuracy_data.append({\n",
    "        'model_name': result['model_name'],\n",
    "        'model_type': 'baseline',\n",
    "        'accuracy': result['baseline']['accuracy']\n",
    "    })\n",
    "    accuracy_data.append({\n",
    "        'model_name': result['model_name'],\n",
    "        'model_type': 'multimodal',\n",
    "        'accuracy': result['multimodal']['accuracy']\n",
    "    })\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  model_type  accuracy\n",
      "0         MobileNetV2    baseline  0.544937\n",
      "1         MobileNetV2  multimodal  0.548350\n",
      "2         InceptionV3    baseline  0.509101\n",
      "3         InceptionV3  multimodal  0.536405\n",
      "4   InceptionResNetV2    baseline  0.485779\n",
      "5   InceptionResNetV2  multimodal  0.513936\n",
      "6            Xception    baseline  0.558589\n",
      "7            Xception  multimodal  0.552901\n",
      "8          ResNet50V2    baseline  0.592150\n",
      "9          ResNet50V2  multimodal  0.608362\n",
      "10     EfficientNetB4    baseline  0.581058\n",
      "11     EfficientNetB4  multimodal  0.524460\n",
      "12        DenseNet121    baseline  0.501422\n",
      "13        DenseNet121  multimodal  0.590728\n",
      "14        DenseNet169    baseline  0.625995\n",
      "15        DenseNet169  multimodal  0.630546\n",
      "16        DenseNet201    baseline  0.515074\n",
      "17        DenseNet201  multimodal  0.624289\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model_name': 'MobileNetV2', 'baseline': {'history': {'accuracy': [0.9652917981147766, 0.979636013507843, 0.9812350273132324, 0.9791656732559204, 0.984480082988739, 0.9834924340248108, 0.9847622513771057, 0.9847622513771057, 0.9863612651824951, 0.9858909845352173, 0.9865493774414062, 0.9859380125999451, 0.9861261248588562, 0.9868316054344177, 0.9859850406646729, 0.9865023493766785, 0.9869256615638733, 0.9865493774414062, 0.986173152923584, 0.9869726896286011], 'loss': [0.12098097801208496, 0.10226666182279587, 0.08671166747808456, 0.1079336628317833, 0.03945382311940193, 0.035444747656583786, 0.026440482586622238, 0.025758834555745125, 0.024002423509955406, 0.023941906169056892, 0.023604637011885643, 0.023658812046051025, 0.023555144667625427, 0.023458819836378098, 0.02347288653254509, 0.023337367922067642, 0.02310115098953247, 0.02298569306731224, 0.023065906018018723, 0.022986404597759247], 'val_accuracy': [0.5795314311981201, 0.6054254174232483, 0.5871351957321167, 0.5926839113235474, 0.6037813425064087, 0.5980271100997925, 0.603164792060852, 0.6017262935638428, 0.6017262935638428, 0.6017262935638428, 0.6021372675895691, 0.6033703088760376, 0.6046033501625061, 0.6035758256912231, 0.6039868593215942, 0.6056309342384338, 0.6048088669776917, 0.6054254174232483, 0.6062474250793457, 0.6070694327354431], 'val_loss': [2.596639633178711, 3.362184762954712, 3.386955976486206, 4.547837257385254, 4.976088047027588, 4.7811455726623535, 4.916506290435791, 4.83119535446167, 4.813974380493164, 4.822068214416504, 4.8212103843688965, 4.820183753967285, 4.8348822593688965, 4.772180080413818, 4.760593891143799, 4.7985405921936035, 4.773280620574951, 4.777842044830322, 4.772754192352295, 4.776718616485596], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5449374288964732, 'conf_matrix': array([[370,  28, 482,  90],\n",
      "       [  0,   0, 209,   0],\n",
      "       [207,   0, 988,  11],\n",
      "       [ 58,   1, 514, 558]])}, 'multimodal': {'history': {'accuracy': [0.9649626016616821, 0.9815642237663269, 0.9801062941551208, 0.9825518727302551, 0.9832102656364441, 0.9843389987945557, 0.9847152233123779, 0.9854677319526672, 0.9857028722763062, 0.9856088161468506, 0.985514760017395, 0.9856088161468506, 0.9859850406646729, 0.9858909845352173, 0.9858439564704895, 0.9863142371177673, 0.9856558442115784, 0.9853736758232117, 0.985514760017395, 0.9859850406646729], 'loss': [0.12218166142702103, 0.09135231375694275, 0.09642896801233292, 0.041479967534542084, 0.03723471984267235, 0.026758650317788124, 0.026143880560994148, 0.024758368730545044, 0.024457750841975212, 0.024370087310671806, 0.02431976981461048, 0.023994600400328636, 0.02422199584543705, 0.024105649441480637, 0.02377891167998314, 0.023711131885647774, 0.02385289967060089, 0.023847727105021477, 0.02374679408967495, 0.02360362932085991], 'val_accuracy': [0.6068639755249023, 0.5743937492370605, 0.5595971941947937, 0.5891903042793274, 0.5920674204826355, 0.592272937297821, 0.58836829662323, 0.5885737538337708, 0.5887792706489563, 0.5871351957321167, 0.5871351957321167, 0.58836829662323, 0.5861076712608337, 0.5852856636047363, 0.5822030305862427, 0.5819975137710571, 0.5852856636047363, 0.5881627798080444, 0.5838471055030823, 0.5828195810317993], 'val_loss': [3.064826011657715, 3.9445221424102783, 4.518529415130615, 4.438908100128174, 4.480687141418457, 4.545302867889404, 4.412641525268555, 4.476881980895996, 4.497182369232178, 4.48046875, 4.474792003631592, 4.501440525054932, 4.421537399291992, 4.434435844421387, 4.432205677032471, 4.369753360748291, 4.447966575622559, 4.469964027404785, 4.455509662628174, 4.441411018371582], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5483503981797497, 'conf_matrix': array([[ 370,    1,  524,   75],\n",
      "       [   0,    0,  209,    0],\n",
      "       [ 171,    0, 1023,   12],\n",
      "       [  42,    0,  554,  535]])}}, {'model_name': 'InceptionV3', 'baseline': {'history': {'accuracy': [0.9212246537208557, 0.9666086435317993, 0.9699007868766785, 0.9766260385513306, 0.9774255752563477, 0.9741334915161133, 0.9808587431907654, 0.9823637008666992, 0.9805295467376709, 0.9813761115074158, 0.981987476348877, 0.982787013053894, 0.9821755886077881, 0.9821285605430603, 0.9816582798957825, 0.9823637008666992, 0.9825048446655273, 0.9822226166725159, 0.9823166728019714, 0.9825048446655273], 'loss': [1.1914639472961426, 0.4904686212539673, 0.5195947289466858, 0.4493035674095154, 0.37957319617271423, 0.5010858774185181, 0.1427702158689499, 0.11587364226579666, 0.10335695743560791, 0.05456892028450966, 0.051946427673101425, 0.04087251052260399, 0.04154939576983452, 0.04011587053537369, 0.04048556461930275, 0.039105139672756195, 0.03877868503332138, 0.03931844234466553, 0.038276076316833496, 0.038229040801525116], 'val_accuracy': [0.4582819640636444, 0.5353472828865051, 0.5172626376152039, 0.5567200779914856, 0.552609920501709, 0.5558980703353882, 0.570694625377655, 0.5380188822746277, 0.5437731146812439, 0.5470612645149231, 0.5495273470878601, 0.5519934296607971, 0.5509659051895142, 0.5517879128456116, 0.5495273470878601, 0.5534319877624512, 0.5538430213928223, 0.5538430213928223, 0.5534319877624512, 0.554048478603363], 'val_loss': [24.922489166259766, 30.220367431640625, 30.26879119873047, 43.174198150634766, 41.33699417114258, 61.35802459716797, 50.82033157348633, 50.645263671875, 50.66478729248047, 50.63877487182617, 50.59415817260742, 50.66054153442383, 50.587860107421875, 50.57847595214844, 50.525386810302734, 50.583763122558594, 50.565433502197266, 50.544925689697266, 50.52880096435547, 50.55427169799805], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5091012514220705, 'conf_matrix': array([[587,   0, 365,  18],\n",
      "       [ 55,   0, 154,   0],\n",
      "       [ 74,   0, 714, 418],\n",
      "       [440,   0, 202, 489]])}, 'multimodal': {'history': {'accuracy': [0.9201429486274719, 0.9703240394592285, 0.9718760251998901, 0.9746978282928467, 0.9759676456451416, 0.9780839681625366, 0.9812820553779602, 0.9824107885360718, 0.9820815324783325, 0.9826459288597107, 0.9822226166725159, 0.9816112518310547, 0.9826459288597107, 0.9820815324783325, 0.9823166728019714, 0.9819404482841492, 0.9818934202194214, 0.9820815324783325, 0.9826459288597107, 0.9823166728019714], 'loss': [1.1828994750976562, 0.3901217579841614, 0.436489462852478, 0.49528491497039795, 0.42227286100387573, 0.5319995880126953, 0.13786111772060394, 0.09996768087148666, 0.11355292797088623, 0.05797305330634117, 0.05077230557799339, 0.04260610044002533, 0.04148115962743759, 0.04108481481671333, 0.040098827332258224, 0.04092089831829071, 0.03948213905096054, 0.03936386480927467, 0.03885459154844284, 0.03853766992688179], 'val_accuracy': [0.5534319877624512, 0.5283600687980652, 0.5567200779914856, 0.5593916773796082, 0.5197287201881409, 0.5462391972541809, 0.5725441575050354, 0.5713111162185669, 0.566789984703064, 0.568228542804718, 0.5684340596199036, 0.568228542804718, 0.568228542804718, 0.5676119923591614, 0.5678175091743469, 0.5676119923591614, 0.568228542804718, 0.5676119923591614, 0.568228542804718, 0.5686395168304443], 'val_loss': [21.066736221313477, 29.97015380859375, 34.33702087402344, 43.653804779052734, 54.038089752197266, 61.61277770996094, 57.09677505493164, 56.88736343383789, 56.05427169799805, 55.90526580810547, 55.90317916870117, 55.9044189453125, 55.8992805480957, 55.8927116394043, 55.90120315551758, 55.90055465698242, 55.89898681640625, 55.904361724853516, 55.91233825683594, 55.918331146240234], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5364050056882821, 'conf_matrix': array([[602,   0, 332,  36],\n",
      "       [ 16,   0, 186,   7],\n",
      "       [ 81,   0, 736, 389],\n",
      "       [110,   0, 473, 548]])}}, {'model_name': 'InceptionResNetV2', 'baseline': {'history': {'accuracy': [0.6602548956871033, 0.7772186398506165, 0.8076471090316772, 0.8292338848114014, 0.9266801476478577, 0.9255043864250183, 0.926821231842041, 0.9273385405540466, 0.9277148246765137, 0.9591308832168579, 0.9573437571525574, 0.9646804332733154, 0.9653858542442322, 0.9650566577911377, 0.965197741985321, 0.9638338685035706, 0.965197741985321, 0.964774489402771, 0.9639749526977539, 0.9659502506256104], 'loss': [18.757301330566406, 11.538273811340332, 10.975399017333984, 10.046323776245117, 2.538231611251831, 2.3215653896331787, 2.143449544906616, 1.9282692670822144, 1.947497010231018, 0.7005947232246399, 0.7020199298858643, 0.45785436034202576, 0.4490105211734772, 0.4484426975250244, 0.4292295575141907, 0.4262520968914032, 0.40719565749168396, 0.4052046835422516, 0.404046893119812, 0.388417512178421], 'val_accuracy': [0.42314016819000244, 0.4504726529121399, 0.42889437079429626, 0.40608301758766174, 0.49609535932540894, 0.5166460871696472, 0.5238388776779175, 0.5026715993881226, 0.5102753639221191, 0.5108919143676758, 0.5127414464950562, 0.5047266483306885, 0.5178791880607605, 0.5133579969406128, 0.5164406299591064, 0.5119194388389587, 0.5125359892845154, 0.5182901620864868, 0.5080147981643677, 0.5150020718574524], 'val_loss': [53.68391799926758, 51.44194030761719, 67.95459747314453, 102.69525909423828, 56.463321685791016, 55.3464469909668, 53.30073928833008, 57.97950744628906, 48.09459686279297, 50.527042388916016, 48.59145736694336, 53.277896881103516, 50.14459228515625, 49.159698486328125, 49.66353988647461, 50.794769287109375, 49.62466049194336, 50.958736419677734, 49.917869567871094, 48.183135986328125], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.4857792946530148, 'conf_matrix': array([[303,  81, 560,  26],\n",
      "       [  1,   0, 207,   1],\n",
      "       [121,   4, 502, 579],\n",
      "       [106,   0, 122, 903]])}, 'multimodal': {'history': {'accuracy': [0.6641113758087158, 0.7783944010734558, 0.8063772916793823, 0.8401448726654053, 0.8507736325263977, 0.9340168237686157, 0.9359921216964722, 0.9392371773719788, 0.9362272620201111, 0.9590838551521301, 0.9612001776695251, 0.9655740261077881, 0.965197741985321, 0.96543288230896, 0.9655740261077881, 0.9650566577911377, 0.966467559337616, 0.9657621383666992, 0.9665145874023438, 0.9669378995895386], 'loss': [17.09796905517578, 11.024773597717285, 10.949350357055664, 9.030462265014648, 9.43245792388916, 2.2068161964416504, 2.0524964332580566, 1.8494527339935303, 1.8793212175369263, 0.8010219931602478, 0.6981099843978882, 0.5112752914428711, 0.49878841638565063, 0.49431276321411133, 0.4861898720264435, 0.481746643781662, 0.46853891015052795, 0.45058587193489075, 0.4407835602760315, 0.4431483745574951], 'val_accuracy': [0.42355117201805115, 0.46485820412635803, 0.47944924235343933, 0.4724619686603546, 0.43978628516197205, 0.5156185626983643, 0.5548705458641052, 0.517673671245575, 0.5067817568778992, 0.5341142416000366, 0.5162351131439209, 0.5203452706336975, 0.5189067125320435, 0.5193176865577698, 0.5236333608627319, 0.5254829525947571, 0.5195232033729553, 0.5158240795135498, 0.519112229347229, 0.5172626376152039], 'val_loss': [40.772064208984375, 56.62630844116211, 69.24127960205078, 58.96461486816406, 77.03153991699219, 65.41262817382812, 60.02008819580078, 61.84674835205078, 59.90718078613281, 60.57013702392578, 62.85408401489258, 58.65465545654297, 56.425418853759766, 59.072479248046875, 59.228965759277344, 57.33026885986328, 59.28477478027344, 57.351558685302734, 58.46465301513672, 57.45884704589844], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5139362912400455, 'conf_matrix': array([[304, 105, 517,  44],\n",
      "       [  4,  18, 187,   0],\n",
      "       [101,   4, 532, 569],\n",
      "       [ 92,   0,  86, 953]])}}, {'model_name': 'Xception', 'baseline': {'history': {'accuracy': [0.9236232042312622, 0.9697126746177673, 0.9726755619049072, 0.9730047583580017, 0.9814231395721436, 0.9806236028671265, 0.9815642237663269, 0.9804354906082153, 0.9819404482841492, 0.9817993640899658, 0.9824107885360718, 0.9825048446655273, 0.9815171957015991, 0.9811879992485046, 0.9822226166725159, 0.9823166728019714, 0.9821285605430603, 0.9816112518310547, 0.9821285605430603, 0.9829751253128052], 'loss': [0.5433489084243774, 0.19180741906166077, 0.18483659625053406, 0.19601553678512573, 0.046755217015743256, 0.043274857103824615, 0.030830929055809975, 0.030262572690844536, 0.02806749753654003, 0.02784457616508007, 0.027380162850022316, 0.02724243700504303, 0.02711089327931404, 0.027326995506882668, 0.02690235897898674, 0.026907330378890038, 0.026928720995783806, 0.026846295222640038, 0.02686714008450508, 0.026553386822342873], 'val_accuracy': [0.489930123090744, 0.5748047828674316, 0.5672009587287903, 0.5302096009254456, 0.5345252752304077, 0.5189067125320435, 0.522605836391449, 0.528976559638977, 0.528976559638977, 0.5293875932693481, 0.5312371850013733, 0.5310316681861877, 0.5328812003135681, 0.5316481590270996, 0.5334977507591248, 0.5322647094726562, 0.5328812003135681, 0.5345252752304077, 0.5339087843894958, 0.5332922339439392], 'val_loss': [11.865760803222656, 11.410456657409668, 14.800495147705078, 19.73464012145996, 17.664409637451172, 17.56087303161621, 17.47484016418457, 17.94769287109375, 17.74823570251465, 17.624011993408203, 17.53904914855957, 17.582366943359375, 17.620420455932617, 17.571672439575195, 17.715068817138672, 17.58248519897461, 17.659971237182617, 17.607717514038086, 17.583650588989258, 17.633255004882812], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.558589306029579, 'conf_matrix': array([[573,  13, 378,   6],\n",
      "       [ 51,   0, 158,   0],\n",
      "       [ 65,   0, 842, 299],\n",
      "       [516,   0,  66, 549]])}, 'multimodal': {'history': {'accuracy': [0.9204722046852112, 0.9675962924957275, 0.9694774746894836, 0.9799181818962097, 0.9731458425521851, 0.97483891248703, 0.9806236028671265, 0.9824578166007996, 0.9814231395721436, 0.9811409711837769, 0.9812820553779602, 0.9820345044136047, 0.9826459288597107, 0.9826929569244385, 0.9806706309318542, 0.9825518727302551, 0.9829280972480774, 0.981987476348877, 0.9816582798957825, 0.9812820553779602], 'loss': [0.5493918061256409, 0.21461758017539978, 0.25206834077835083, 0.11711084842681885, 0.20528052747249603, 0.22727973759174347, 0.04286790266633034, 0.040231768041849136, 0.038630831986665726, 0.03840012848377228, 0.029125725850462914, 0.0281459279358387, 0.026519598439335823, 0.02628900483250618, 0.026397401466965675, 0.02614588476717472, 0.026226745918393135, 0.026327747851610184, 0.02628365345299244, 0.0263693667948246], 'val_accuracy': [0.5369913578033447, 0.545622706413269, 0.5565146207809448, 0.5704891085624695, 0.5413070321083069, 0.5380188822746277, 0.5550760626792908, 0.5793259143829346, 0.5764488577842712, 0.568228542804718, 0.570694625377655, 0.5721331834793091, 0.5723387002944946, 0.5739827156066895, 0.5739827156066895, 0.574188232421875, 0.5768598318099976, 0.5768598318099976, 0.5774763822555542, 0.5772708654403687], 'val_loss': [9.215608596801758, 13.239066123962402, 15.579940795898438, 14.559200286865234, 19.462919235229492, 24.502342224121094, 20.125370025634766, 20.58274269104004, 20.68842124938965, 20.13724136352539, 20.289379119873047, 20.3564510345459, 20.375289916992188, 20.45490837097168, 20.448957443237305, 20.443723678588867, 20.425825119018555, 20.514572143554688, 20.586076736450195, 20.530532836914062], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.552901023890785, 'conf_matrix': array([[548,  42, 373,   7],\n",
      "       [ 52,   0, 157,   0],\n",
      "       [ 77,  16, 809, 304],\n",
      "       [500,   0,  44, 587]])}}, {'model_name': 'ResNet50V2', 'baseline': {'history': {'accuracy': [0.9334054589271545, 0.9676903486251831, 0.9764849543571472, 0.977566659450531, 0.9783661961555481, 0.9825518727302551, 0.9823637008666992, 0.9815171957015991, 0.981752336025238, 0.9820815324783325, 0.9833513498306274, 0.9835394620895386, 0.9831632375717163, 0.9826459288597107, 0.9825518727302551, 0.9837275743484497, 0.9832102656364441, 0.9825989007949829, 0.9842919707298279, 0.9828810691833496], 'loss': [2.624175786972046, 1.9133399724960327, 1.7023906707763672, 2.0926647186279297, 2.3972561359405518, 0.5195073485374451, 0.45364895462989807, 0.4789559841156006, 0.4420757293701172, 0.17319080233573914, 0.16927146911621094, 0.15734419226646423, 0.11103693395853043, 0.11169672757387161, 0.1109805777668953, 0.10959059745073318, 0.1097591370344162, 0.10498691350221634, 0.09999584406614304, 0.10330694913864136], 'val_accuracy': [0.5495273470878601, 0.517673671245575, 0.5542539954185486, 0.5166460871696472, 0.5521989464759827, 0.5863131880760193, 0.5891903042793274, 0.5873407125473022, 0.5889847874641418, 0.5916563868522644, 0.5887792706489563, 0.5902178287506104, 0.5891903042793274, 0.5889847874641418, 0.5889847874641418, 0.5893958210945129, 0.5904233455657959, 0.590834379196167, 0.5906288623809814, 0.5902178287506104], 'val_loss': [41.555580139160156, 104.57341003417969, 73.18354034423828, 92.17121887207031, 91.515625, 83.02880859375, 80.60636138916016, 81.99539947509766, 81.96495819091797, 80.02205657958984, 81.48243713378906, 80.3985595703125, 80.81873321533203, 80.762939453125, 80.85814666748047, 81.15453338623047, 80.5176773071289, 80.65021514892578, 80.75834655761719, 80.98596954345703], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5921501706484642, 'conf_matrix': array([[575, 197,  51, 147],\n",
      "       [  0,   0, 209,   0],\n",
      "       [180,  25, 520, 481],\n",
      "       [ 53,   0,  91, 987]])}, 'multimodal': {'history': {'accuracy': [0.9334524869918823, 0.9712176322937012, 0.9753562808036804, 0.9783191680908203, 0.975074052810669, 0.981987476348877, 0.9778017997741699, 0.9824107885360718, 0.9820815324783325, 0.9832572937011719, 0.9818934202194214, 0.9826459288597107, 0.9833043217658997, 0.982787013053894, 0.9833043217658997, 0.9830691814422607, 0.9823166728019714, 0.9826929569244385, 0.9828810691833496, 0.982787013053894], 'loss': [2.7751832008361816, 1.761146903038025, 1.8840854167938232, 1.7964075803756714, 2.134852170944214, 1.6495153903961182, 1.9113701581954956, 0.45538023114204407, 0.4707595109939575, 0.17260342836380005, 0.15830223262310028, 0.11435149610042572, 0.11112318187952042, 0.1110268384218216, 0.11042089015245438, 0.10312428325414658, 0.10591097921133041, 0.10257518291473389, 0.10040369629859924, 0.09732648730278015], 'val_accuracy': [0.5505548715591431, 0.5577476620674133, 0.5310316681861877, 0.573160707950592, 0.5793259143829346, 0.5632963180541992, 0.5575421452522278, 0.575626790523529, 0.5700780749320984, 0.5674064755439758, 0.5577476620674133, 0.5614467859268188, 0.5577476620674133, 0.5581586360931396, 0.5581586360931396, 0.5550760626792908, 0.5565146207809448, 0.5556925535202026, 0.5556925535202026, 0.5554870367050171], 'val_loss': [42.9754753112793, 105.74607849121094, 68.25367736816406, 105.89094543457031, 92.60025024414062, 92.88240051269531, 126.1439437866211, 117.96416473388672, 115.6470947265625, 115.5361557006836, 114.98686981201172, 115.16716766357422, 114.9483642578125, 114.99201202392578, 114.96123504638672, 114.692626953125, 114.74787139892578, 114.69316101074219, 114.63809204101562, 114.61756134033203], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.6083617747440273, 'conf_matrix': array([[ 547,   24,   90,  309],\n",
      "       [   0,    0,  209,    0],\n",
      "       [ 173,   77,  548,  408],\n",
      "       [  47,    0,   40, 1044]])}}, {'model_name': 'EfficientNetB4', 'baseline': {'history': {'accuracy': [0.9644452929496765, 0.9816112518310547, 0.9787894487380981, 0.9828810691833496, 0.9824107885360718, 0.9823166728019714, 0.9827399849891663, 0.9819404482841492, 0.9824107885360718, 0.981329083442688, 0.9823637008666992, 0.9810939431190491, 0.9815171957015991, 0.9812820553779602, 0.9813761115074158, 0.9822226166725159, 0.9823166728019714, 0.9825989007949829, 0.9819404482841492, 0.9816582798957825], 'loss': [0.12312157452106476, 0.08388343453407288, 0.10849946737289429, 0.04460075870156288, 0.038396209478378296, 0.03131205588579178, 0.030686030164361, 0.02897554636001587, 0.028261946514248848, 0.0293362345546484, 0.028440603986382484, 0.029789337888360023, 0.030040515586733818, 0.02907990850508213, 0.029062405228614807, 0.028727257624268532, 0.027810391038656235, 0.028087327256798744, 0.028372716158628464, 0.028236232697963715], 'val_accuracy': [0.6539251804351807, 0.6446773409843445, 0.620838463306427, 0.6356350183486938, 0.6341964602470398, 0.6368680596351624, 0.6309083700180054, 0.6315248608589172, 0.6327579021453857, 0.6333744525909424, 0.6356350183486938, 0.6356350183486938, 0.6354295015335083, 0.6376900672912598, 0.6358405351638794, 0.637484610080719, 0.6372790932655334, 0.6368680596351624, 0.6352239847183228, 0.6354295015335083], 'val_loss': [1.3805474042892456, 1.7202506065368652, 2.6076529026031494, 2.4395854473114014, 2.305351495742798, 2.2494499683380127, 2.254845380783081, 2.3101718425750732, 2.3710150718688965, 2.4051661491394043, 2.3594138622283936, 2.358403444290161, 2.331270456314087, 2.3716979026794434, 2.3471062183380127, 2.378483772277832, 2.3826684951782227, 2.3926777839660645, 2.354719877243042, 2.359240770339966], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5810580204778157, 'conf_matrix': array([[683,  81,  89, 117],\n",
      "       [  0,   0, 209,   0],\n",
      "       [ 39,   0, 689, 478],\n",
      "       [  1,   6, 453, 671]])}, 'multimodal': {'history': {'accuracy': [0.9660443067550659, 0.9808117151260376, 0.9796830415725708, 0.9818934202194214, 0.9815642237663269, 0.9836335182189941, 0.9817993640899658, 0.981752336025238, 0.9818463921546936, 0.9816112518310547, 0.981987476348877, 0.982787013053894, 0.9816582798957825, 0.981752336025238, 0.9825989007949829, 0.9821755886077881, 0.9830691814422607, 0.9827399849891663, 0.9819404482841492, 0.9816112518310547], 'loss': [0.11415276676416397, 0.08834543079137802, 0.09585704654455185, 0.045516207814216614, 0.039397869259119034, 0.03027096576988697, 0.03035680763423443, 0.029574714601039886, 0.029207298532128334, 0.029055031016469002, 0.02898561581969261, 0.028709901496767998, 0.028887523338198662, 0.02702564001083374, 0.028101131319999695, 0.028002945706248283, 0.027229249477386475, 0.027417294681072235, 0.028919890522956848, 0.02879038266837597], 'val_accuracy': [0.6368680596351624, 0.5834360718727112, 0.6056309342384338, 0.6083025336265564, 0.6255651712417603, 0.6212494969367981, 0.622277021408081, 0.620838463306427, 0.620838463306427, 0.620838463306427, 0.6214550137519836, 0.6196054220199585, 0.620838463306427, 0.62186598777771, 0.6226880550384521, 0.6204274296760559, 0.623304545879364, 0.6216605305671692, 0.620838463306427, 0.6187834143638611], 'val_loss': [1.7857578992843628, 2.4008822441101074, 2.7495980262756348, 2.5703957080841064, 2.363723039627075, 2.2657580375671387, 2.3024508953094482, 2.302961587905884, 2.3238706588745117, 2.3221826553344727, 2.312152862548828, 2.3187170028686523, 2.341742992401123, 2.3293938636779785, 2.2640600204467773, 2.3042430877685547, 2.319255828857422, 2.303144693374634, 2.299353837966919, 2.2770087718963623], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5244596131968146, 'conf_matrix': array([[496, 243, 143,  88],\n",
      "       [  0,   5, 204,   0],\n",
      "       [ 36,   0, 731, 439],\n",
      "       [  0,   3, 516, 612]])}}, {'model_name': 'DenseNet121', 'baseline': {'history': {'accuracy': [0.9588016867637634, 0.9793067574501038, 0.9807646870613098, 0.9797770977020264, 0.9815642237663269, 0.9828340411186218, 0.9830691814422607, 0.9846681952476501, 0.9850914478302002, 0.9862672090530396, 0.9857028722763062, 0.9856558442115784, 0.9856088161468506, 0.9857969284057617, 0.986173152923584, 0.9862201809883118, 0.9862201809883118, 0.986596405506134, 0.9857969284057617, 0.9862672090530396], 'loss': [0.1540558636188507, 0.10217399150133133, 0.1132703572511673, 0.1107436791062355, 0.1057625412940979, 0.041703470051288605, 0.03947944566607475, 0.026595255360007286, 0.02585677243769169, 0.02445298433303833, 0.024251386523246765, 0.024160150438547134, 0.02416079305112362, 0.024224260821938515, 0.02400362305343151, 0.02398894913494587, 0.023816833272576332, 0.023921528831124306, 0.02385348454117775, 0.02362118847668171], 'val_accuracy': [0.5770653486251831, 0.551582396030426, 0.6504315733909607, 0.6144677400588989, 0.5988491773605347, 0.6487874984741211, 0.6500205397605896, 0.6489930152893066, 0.64837646484375, 0.6494040489196777, 0.6496095061302185, 0.64837646484375, 0.6487874984741211, 0.64837646484375, 0.6461158990859985, 0.6461158990859985, 0.6477599740028381, 0.6477599740028381, 0.6469379663467407, 0.6465269327163696], 'val_loss': [3.132780075073242, 4.734888553619385, 5.640096664428711, 6.945223331451416, 5.786055088043213, 6.253249168395996, 6.444255352020264, 6.370384216308594, 6.364060401916504, 6.393254280090332, 6.418395042419434, 6.3484086990356445, 6.374227523803711, 6.395510196685791, 6.2792582511901855, 6.3240203857421875, 6.405533790588379, 6.303065299987793, 6.290241241455078, 6.369300365447998], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5014220705346986, 'conf_matrix': array([[351,   4, 278, 337],\n",
      "       [  0,   0, 209,   0],\n",
      "       [113,   0, 667, 426],\n",
      "       [104,   0, 282, 745]])}, 'multimodal': {'history': {'accuracy': [0.9601185321807861, 0.9788835048675537, 0.9808117151260376, 0.9795418977737427, 0.9825518727302551, 0.9829751253128052, 0.984480082988739, 0.9846681952476501, 0.9852325916290283, 0.9849503636360168, 0.9851855039596558, 0.9842919707298279, 0.9847152233123779, 0.9848563075065613, 0.9853266477584839, 0.9847152233123779, 0.9854677319526672, 0.985514760017395, 0.985138475894928, 0.9851855039596558], 'loss': [0.14901216328144073, 0.09908533841371536, 0.10338521003723145, 0.12136433273553848, 0.04253869876265526, 0.037843380123376846, 0.027165107429027557, 0.02660256437957287, 0.024917764589190483, 0.024715758860111237, 0.024605540558695793, 0.024759573861956596, 0.024429861456155777, 0.02437969110906124, 0.024299977347254753, 0.024367203935980797, 0.02423902228474617, 0.024108801037073135, 0.024070153012871742, 0.024035919457674026], 'val_accuracy': [0.620838463306427, 0.6438553333282471, 0.603164792060852, 0.6085079908370972, 0.6161118149757385, 0.6245375871658325, 0.61487877368927, 0.61487877368927, 0.6154952645301819, 0.6128236651420593, 0.6128236651420593, 0.6093300580978394, 0.6099465489387512, 0.6087135076522827, 0.6089190244674683, 0.6089190244674683, 0.6087135076522827, 0.6074804663658142, 0.6093300580978394, 0.6083025336265564], 'val_loss': [2.6876790523529053, 3.9294068813323975, 5.277647018432617, 6.28842830657959, 6.278749465942383, 6.336297512054443, 6.251806259155273, 6.266899585723877, 6.270767688751221, 6.257009506225586, 6.2744646072387695, 6.279510974884033, 6.290904521942139, 6.288028717041016, 6.283283710479736, 6.296298027038574, 6.330878257751465, 6.348278522491455, 6.414195537567139, 6.374931335449219], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5907281001137656, 'conf_matrix': array([[387,  50, 278, 255],\n",
      "       [  0,   0, 209,   0],\n",
      "       [  0,   0, 792, 414],\n",
      "       [ 36,   8, 189, 898]])}}, {'model_name': 'DenseNet169', 'baseline': {'history': {'accuracy': [0.9598363637924194, 0.9769552946090698, 0.980952799320221, 0.9773785471916199, 0.9788835048675537, 0.983022153377533, 0.9821285605430603, 0.9845741391181946, 0.9833513498306274, 0.9847622513771057, 0.9846681952476501, 0.984480082988739, 0.9844330549240112, 0.9856088161468506, 0.9846211671829224, 0.985138475894928, 0.9854677319526672, 0.9857499003410339, 0.9853266477584839, 0.9849503636360168], 'loss': [0.15153728425502777, 0.13646762073040009, 0.13225384056568146, 0.13814887404441833, 0.1539708971977234, 0.043068572878837585, 0.04011479392647743, 0.02793005295097828, 0.02758665569126606, 0.025516334921121597, 0.025367816910147667, 0.025143975391983986, 0.025044817477464676, 0.024636350572109222, 0.025131698697805405, 0.0249274093657732, 0.02470705285668373, 0.024752067402005196, 0.02473541349172592, 0.024759816005825996], 'val_accuracy': [0.5458282232284546, 0.5832305550575256, 0.6029593348503113, 0.5641183853149414, 0.5388409495353699, 0.5676119923591614, 0.5698725581169128, 0.5645293593406677, 0.5604192614555359, 0.5630908608436584, 0.5645293593406677, 0.5649403929710388, 0.5628853440284729, 0.5649403929710388, 0.5649403929710388, 0.5651459097862244, 0.5669955015182495, 0.5661734342575073, 0.5647348761558533, 0.5649403929710388], 'val_loss': [4.783838748931885, 8.103157997131348, 7.950284004211426, 9.712630271911621, 11.862396240234375, 10.719746589660645, 10.666703224182129, 10.80865478515625, 10.99127197265625, 10.889495849609375, 10.856523513793945, 10.863118171691895, 10.893903732299805, 10.843620300292969, 10.870911598205566, 10.881548881530762, 10.848241806030273, 10.827174186706543, 10.88367748260498, 10.896160125732422], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.625995449374289, 'conf_matrix': array([[514,   0, 231, 225],\n",
      "       [  4,  21, 184,   0],\n",
      "       [  0,   1, 736, 469],\n",
      "       [ 15,  10, 176, 930]])}, 'multimodal': {'history': {'accuracy': [0.961388349533081, 0.9784132242202759, 0.9811409711837769, 0.9775196313858032, 0.9826459288597107, 0.9829280972480774, 0.9836335182189941, 0.9831162095069885, 0.9829280972480774, 0.984056830406189, 0.9830691814422607, 0.9841038584709167, 0.9837746024131775, 0.9839627742767334, 0.9837746024131775, 0.98382169008255, 0.9848563075065613, 0.9848563075065613, 0.9842919707298279, 0.9832572937011719], 'loss': [0.15353475511074066, 0.11526831239461899, 0.11883417516946793, 0.15912321209907532, 0.05084596946835518, 0.044314730912446976, 0.02915346808731556, 0.028464123606681824, 0.02833278849720955, 0.025961417704820633, 0.02598283439874649, 0.025807229802012444, 0.025639168918132782, 0.025888381525874138, 0.025593535974621773, 0.02582123503088951, 0.025183547288179398, 0.02522498369216919, 0.025486694648861885, 0.025245556607842445], 'val_accuracy': [0.5267159938812256, 0.574188232421875, 0.5474722385406494, 0.5690505504608154, 0.5674064755439758, 0.5735717415809631, 0.575626790523529, 0.5743937492370605, 0.574188232421875, 0.5745992660522461, 0.5743937492370605, 0.5733662247657776, 0.572749674320221, 0.574188232421875, 0.5735717415809631, 0.5739827156066895, 0.574188232421875, 0.5743937492370605, 0.5748047828674316, 0.5750102996826172], 'val_loss': [3.863668203353882, 6.349503040313721, 6.397584438323975, 9.813699722290039, 9.567261695861816, 9.64847469329834, 9.564821243286133, 9.487689971923828, 9.44176959991455, 9.526876449584961, 9.487257957458496, 9.594407081604004, 9.628316879272461, 9.558470726013184, 9.567999839782715, 9.630087852478027, 9.63166618347168, 9.748601913452148, 9.709757804870605, 9.746481895446777], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.6305460750853242, 'conf_matrix': array([[680,   0, 120, 170],\n",
      "       [  0,  53, 156,   0],\n",
      "       [155,   0, 569, 482],\n",
      "       [ 57,  14, 145, 915]])}}, {'model_name': 'DenseNet201', 'baseline': {'history': {'accuracy': [0.9628933072090149, 0.9794948697090149, 0.9791656732559204, 0.9804825186729431, 0.981752336025238, 0.9827399849891663, 0.9831162095069885, 0.9828810691833496, 0.9840098023414612, 0.9833513498306274, 0.9835864901542664, 0.9833983778953552, 0.9844330549240112, 0.9833043217658997, 0.9838687181472778, 0.9840098023414612, 0.9840098023414612, 0.9838687181472778, 0.9837746024131775, 0.9841979146003723], 'loss': [0.14153453707695007, 0.09259600937366486, 0.12656161189079285, 0.12557606399059296, 0.047292668372392654, 0.039877913892269135, 0.0276937372982502, 0.02814810536801815, 0.025808630511164665, 0.02594088576734066, 0.02578211948275566, 0.025718770921230316, 0.025761373341083527, 0.025678953155875206, 0.025683434680104256, 0.02546342834830284, 0.02548680268228054, 0.025382371619343758, 0.025229915976524353, 0.025280267000198364], 'val_accuracy': [0.5659679174423218, 0.5963830947875977, 0.543156623840332, 0.5600082278251648, 0.5887792706489563, 0.5873407125473022, 0.5889847874641418, 0.5893958210945129, 0.5881627798080444, 0.5881627798080444, 0.5879572629928589, 0.5891903042793274, 0.5881627798080444, 0.5885737538337708, 0.58836829662323, 0.5885737538337708, 0.58836829662323, 0.58836829662323, 0.5898067951202393, 0.5889847874641418], 'val_loss': [9.589107513427734, 6.990087509155273, 9.122530937194824, 11.086261749267578, 10.315837860107422, 10.197014808654785, 10.324383735656738, 10.290789604187012, 10.225140571594238, 10.179027557373047, 10.152653694152832, 10.243680000305176, 10.114668846130371, 10.144448280334473, 10.112894058227539, 10.116246223449707, 10.123237609863281, 10.160479545593262, 10.131110191345215, 10.195557594299316], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.5150739476678043, 'conf_matrix': array([[576,   0, 155, 239],\n",
      "       [  1,  29, 179,   0],\n",
      "       [  0,  20, 665, 521],\n",
      "       [123,   2, 465, 541]])}, 'multimodal': {'history': {'accuracy': [0.9608709812164307, 0.976531982421875, 0.9808587431907654, 0.980952799320221, 0.9806706309318542, 0.9832102656364441, 0.9829280972480774, 0.9836335182189941, 0.9833983778953552, 0.9841508865356445, 0.9852325916290283, 0.9841038584709167, 0.984056830406189, 0.9849033355712891, 0.9842919707298279, 0.9841979146003723, 0.9840098023414612, 0.9841979146003723, 0.9835394620895386, 0.9841508865356445], 'loss': [0.13967803120613098, 0.11245311796665192, 0.10916672646999359, 0.10978395491838455, 0.12074019014835358, 0.04618263244628906, 0.03956964612007141, 0.02752537466585636, 0.027048245072364807, 0.025210777297616005, 0.024678368121385574, 0.025186719372868538, 0.02499081753194332, 0.024733226746320724, 0.024705974385142326, 0.024721836671233177, 0.024710219353437424, 0.024589793756604195, 0.024690866470336914, 0.0244863610714674], 'val_accuracy': [0.6177558302879333, 0.5935059785842896, 0.6237155795097351, 0.5489107966423035, 0.5893958210945129, 0.5838471055030823, 0.5822030305862427, 0.5838471055030823, 0.5838471055030823, 0.5842581391334534, 0.5861076712608337, 0.5844635963439941, 0.5863131880760193, 0.5852856636047363, 0.5865187048912048, 0.5867242217063904, 0.5850801467895508, 0.5859021544456482, 0.5856966972351074, 0.5871351957321167], 'val_loss': [4.849746227264404, 10.38143539428711, 7.1777496337890625, 10.49185848236084, 14.823141098022461, 12.764938354492188, 12.365350723266602, 12.211472511291504, 12.147403717041016, 12.12540340423584, 11.888834953308105, 12.205438613891602, 12.05237102508545, 12.154522895812988, 11.974512100219727, 11.980634689331055, 11.976958274841309, 11.983601570129395, 12.01005744934082, 12.028583526611328], 'learning_rate': [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07, 9.999999974752427e-07]}, 'accuracy': 0.6242889647326507, 'conf_matrix': array([[ 604,    0,  221,  145],\n",
      "       [   4,   45,  160,    0],\n",
      "       [ 163,    0,  537,  506],\n",
      "       [  69,   13,   40, 1009]])}}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and loading the baseline model for MobileNetV2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from ./model_weights/MobileNetV2_baseline.weights.h5\n",
      "Building and loading the multimodal model for MobileNetV2...\n",
      "Loaded weights from ./model_weights/MobileNetV2_multimodal.weights.h5\n",
      "\n",
      "Summary for Baseline Model (MobileNetV2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_37\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_37\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " cnn_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " mobilenetv2_1.00_224             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                                                           \n",
       "\n",
       " flatten_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62720</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">250,884</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " cnn_input (\u001b[38;5;33mInputLayer\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " mobilenetv2_1.00_224             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          \u001b[38;5;34m2,257,984\u001b[0m \n",
       " (\u001b[38;5;33mFunctional\u001b[0m)                                                           \n",
       "\n",
       " flatten_18 (\u001b[38;5;33mFlatten\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62720\u001b[0m)                       \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_45 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                     \u001b[38;5;34m250,884\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,868</span> (9.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,508,868\u001b[0m (9.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,884</span> (980.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m250,884\u001b[0m (980.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Multimodal Model (MobileNetV2):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_39\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_39\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " probe_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  probe_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       " dropout_18           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
       "\n",
       " cnn_input            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                                               \n",
       "\n",
       " dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span>  dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "\n",
       " mobilenetv2_1.00_2  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,        <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span>  cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                                            \n",
       "\n",
       " dropout_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
       "\n",
       " flatten_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62720</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mobilenetv2_1.00 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                                                             \n",
       "\n",
       " dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span>  dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "\n",
       " concatenate_13       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63232</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">252,932</span>  concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " probe_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " dense_46 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)            \u001b[38;5;34m3,072\u001b[0m  probe_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n",
       " dropout_18           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                \u001b[38;5;34m0\u001b[0m  dense_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
       "\n",
       " cnn_input            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,            \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m3\u001b[0m)                                               \n",
       "\n",
       " dense_47 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        \u001b[38;5;34m1,049,600\u001b[0m  dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "\n",
       " mobilenetv2_1.00_2  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,        \u001b[38;5;34m2,257,984\u001b[0m  cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mFunctional\u001b[0m)         \u001b[38;5;34m1280\u001b[0m)                                            \n",
       "\n",
       " dropout_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                \u001b[38;5;34m0\u001b[0m  dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
       "\n",
       " flatten_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62720\u001b[0m)               \u001b[38;5;34m0\u001b[0m  mobilenetv2_1.00 \n",
       " (\u001b[38;5;33mFlatten\u001b[0m)                                                             \n",
       "\n",
       " dense_48 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           \u001b[38;5;34m524,800\u001b[0m  dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "\n",
       " concatenate_13       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63232\u001b[0m)               \u001b[38;5;34m0\u001b[0m  flatten_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], \n",
       " (\u001b[38;5;33mConcatenate\u001b[0m)                                       dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " dense_49 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)             \u001b[38;5;34m252,932\u001b[0m  concatenate_13[\u001b[38;5;34m0\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,088,388</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,088,388\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,830,404</span> (6.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,830,404\u001b[0m (6.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_model_summaries(base_model_name, base_model_class):\n",
    "    # Define paths for model weights\n",
    "    baseline_weights_path = f'./model_weights/{base_model_name}_baseline.weights.h5'\n",
    "    multimodal_weights_path = f'./model_weights/{base_model_name}_multimodal.weights.h5'\n",
    "    \n",
    "    # Build and load the baseline model\n",
    "    print(f\"Building and loading the baseline model for {base_model_name}...\")\n",
    "    baseline_model = build_baseline_model(base_model_class)\n",
    "    if os.path.exists(baseline_weights_path):\n",
    "        baseline_model.load_weights(baseline_weights_path)\n",
    "        print(f\"Loaded weights from {baseline_weights_path}\")\n",
    "    else:\n",
    "        print(f\"Weights file for baseline model not found at {baseline_weights_path}\")\n",
    "    \n",
    "    # Build and load the multimodal model\n",
    "    print(f\"Building and loading the multimodal model for {base_model_name}...\")\n",
    "    multimodal_model = build_multimodal_model(base_model_class)\n",
    "    if os.path.exists(multimodal_weights_path):\n",
    "        multimodal_model.load_weights(multimodal_weights_path)\n",
    "        print(f\"Loaded weights from {multimodal_weights_path}\")\n",
    "    else:\n",
    "        print(f\"Weights file for multimodal model not found at {multimodal_weights_path}\")\n",
    "    \n",
    "    # Print summaries\n",
    "    print(f\"\\nSummary for Baseline Model ({base_model_name}):\")\n",
    "    baseline_model.summary()\n",
    "    \n",
    "    print(f\"\\nSummary for Multimodal Model ({base_model_name}):\")\n",
    "    multimodal_model.summary()\n",
    "\n",
    "print_model_summaries(\"MobileNetV2\",MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = np.mean(y_true == y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_trained_models(models, X_test, y_test, probe_test, results):\n",
    "#     evaluation_data = []\n",
    "    \n",
    "#     for model_name, model_class in models.items():\n",
    "#         # Define paths for model weights\n",
    "#         baseline_weights_path = f'./model_weights/{model_name}_baseline.weights.h5'\n",
    "#         multimodal_weights_path = f'./model_weights/{model_name}_multimodal.weights.h5'\n",
    "        \n",
    "#         # Evaluate baseline model\n",
    "#         print(f'Evaluating {model_name} Baseline Model')\n",
    "#         baseline_model = build_baseline_model(model_class)\n",
    "#         if os.path.exists(baseline_weights_path):\n",
    "#             baseline_model.load_weights(baseline_weights_path)\n",
    "#             y_pred_baseline = np.argmax(baseline_model.predict(X_test), axis=1)\n",
    "#             accuracy_baseline, precision_baseline, recall_baseline, f1_baseline = calculate_metrics(y_test, y_pred_baseline)\n",
    "#             params_baseline = baseline_model.count_params()\n",
    "#         else:\n",
    "#             print(f\"Weights for baseline model not found at {baseline_weights_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         # Evaluate multimodal model\n",
    "#         print(f'Evaluating {model_name} Multimodal Model')\n",
    "#         multimodal_model = build_multimodal_model(model_class)\n",
    "#         if os.path.exists(multimodal_weights_path):\n",
    "#             multimodal_model.load_weights(multimodal_weights_path)\n",
    "#             y_pred_multimodal = np.argmax(multimodal_model.predict([X_test, probe_test]), axis=1)\n",
    "#             accuracy_multimodal, precision_multimodal, recall_multimodal, f1_multimodal = calculate_metrics(y_test, y_pred_multimodal)\n",
    "#             params_multimodal = multimodal_model.count_params()\n",
    "#         else:\n",
    "#             print(f\"Weights for multimodal model not found at {multimodal_weights_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         evaluation_data.append({\n",
    "#             \"model_name\": model_name,\n",
    "#             \"baseline_params\": params_baseline,\n",
    "#             \"baseline_accuracy\": accuracy_baseline,\n",
    "#             \"baseline_precision\": precision_baseline,\n",
    "#             \"baseline_recall\": recall_baseline,\n",
    "#             \"baseline_f1\": f1_baseline,\n",
    "#             \"multimodal_params\": params_multimodal,\n",
    "#             \"multimodal_accuracy\": accuracy_multimodal,\n",
    "#             \"multimodal_precision\": precision_multimodal,\n",
    "#             \"multimodal_recall\": recall_multimodal,\n",
    "#             \"multimodal_f1\": f1_multimodal\n",
    "#         })\n",
    "    \n",
    "#     return evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_results = evaluate_trained_models(models, X_test, y_test, probe_test, results)\n",
    "\n",
    "# # Print the evaluation results in the desired format\n",
    "# import pandas as pd\n",
    "\n",
    "# df_results = pd.DataFrame(evaluation_results)\n",
    "# print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = np.mean(y_true == y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate_trained_models(models, X_test, y_test, probe_test):\n",
    "    evaluation_data = []\n",
    "    \n",
    "    for model_name, model_class in models.items():\n",
    "        # Define paths for model weights\n",
    "        baseline_weights_path = f'./model_weights/{model_name}_baseline.weights.h5'\n",
    "        multimodal_weights_path = f'./model_weights/{model_name}_multimodal.weights.h5'\n",
    "        \n",
    "        # Evaluate baseline model\n",
    "        print(f'Evaluating {model_name} Baseline Model')\n",
    "        baseline_model = build_baseline_model(model_class)\n",
    "        if os.path.exists(baseline_weights_path):\n",
    "            baseline_model.load_weights(baseline_weights_path)\n",
    "            y_pred_baseline = np.argmax(baseline_model.predict(X_test), axis=1)\n",
    "            accuracy_baseline, precision_baseline, recall_baseline, f1_baseline = calculate_metrics(y_test, y_pred_baseline)\n",
    "            params_baseline = baseline_model.count_params()\n",
    "            \n",
    "            evaluation_data.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"model_type\": \"baseline\",\n",
    "                \"params\": params_baseline,\n",
    "                \"accuracy\": accuracy_baseline,\n",
    "                \"precision\": precision_baseline,\n",
    "                \"recall\": recall_baseline,\n",
    "                \"f1\": f1_baseline\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Weights for baseline model not found at {baseline_weights_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate multimodal model\n",
    "        print(f'Evaluating {model_name} Multimodal Model')\n",
    "        multimodal_model = build_multimodal_model(model_class)\n",
    "        if os.path.exists(multimodal_weights_path):\n",
    "            multimodal_model.load_weights(multimodal_weights_path)\n",
    "            y_pred_multimodal = np.argmax(multimodal_model.predict([X_test, probe_test]), axis=1)\n",
    "            accuracy_multimodal, precision_multimodal, recall_multimodal, f1_multimodal = calculate_metrics(y_test, y_pred_multimodal)\n",
    "            params_multimodal = multimodal_model.count_params()\n",
    "            \n",
    "            evaluation_data.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"model_type\": \"multimodal\",\n",
    "                \"params\": params_multimodal,\n",
    "                \"accuracy\": accuracy_multimodal,\n",
    "                \"precision\": precision_multimodal,\n",
    "                \"recall\": recall_multimodal,\n",
    "                \"f1\": f1_multimodal\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Weights for multimodal model not found at {multimodal_weights_path}\")\n",
    "            continue\n",
    "    \n",
    "    df_results = pd.DataFrame(evaluation_data)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MobileNetV2 Baseline Model\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 149ms/step\n",
      "Evaluating MobileNetV2 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 151ms/step\n",
      "Evaluating InceptionV3 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 345ms/step\n",
      "Evaluating InceptionV3 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 342ms/step\n",
      "Evaluating InceptionResNetV2 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 783ms/step\n",
      "Evaluating InceptionResNetV2 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 814ms/step\n",
      "Evaluating Xception Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 617ms/step\n",
      "Evaluating Xception Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 612ms/step\n",
      "Evaluating ResNet50V2 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 473ms/step\n",
      "Evaluating ResNet50V2 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 467ms/step\n",
      "Evaluating EfficientNetB4 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 642ms/step\n",
      "Evaluating EfficientNetB4 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 605ms/step\n",
      "Evaluating DenseNet121 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 498ms/step\n",
      "Evaluating DenseNet121 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 493ms/step\n",
      "Evaluating DenseNet169 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 608ms/step\n",
      "Evaluating DenseNet169 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 603ms/step\n",
      "Evaluating DenseNet201 Baseline Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 786ms/step\n",
      "Evaluating DenseNet201 Multimodal Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonc/Desktop/Project/venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 790ms/step\n"
     ]
    }
   ],
   "source": [
    "df_results = evaluate_trained_models(models, X_test, y_test, probe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>params</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>baseline</td>\n",
       "      <td>2508868</td>\n",
       "      <td>54.493743</td>\n",
       "      <td>58.765340</td>\n",
       "      <td>54.493743</td>\n",
       "      <td>52.715298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>4088388</td>\n",
       "      <td>54.835040</td>\n",
       "      <td>60.366897</td>\n",
       "      <td>54.835040</td>\n",
       "      <td>52.739745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>baseline</td>\n",
       "      <td>22007588</td>\n",
       "      <td>50.910125</td>\n",
       "      <td>48.080548</td>\n",
       "      <td>50.910125</td>\n",
       "      <td>49.082164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>23587108</td>\n",
       "      <td>53.640501</td>\n",
       "      <td>53.134401</td>\n",
       "      <td>53.640501</td>\n",
       "      <td>52.586495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>baseline</td>\n",
       "      <td>54490340</td>\n",
       "      <td>48.577929</td>\n",
       "      <td>47.370282</td>\n",
       "      <td>48.577929</td>\n",
       "      <td>46.404017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>56069860</td>\n",
       "      <td>51.393629</td>\n",
       "      <td>50.961373</td>\n",
       "      <td>51.393629</td>\n",
       "      <td>49.209224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xception</td>\n",
       "      <td>baseline</td>\n",
       "      <td>21262892</td>\n",
       "      <td>55.858931</td>\n",
       "      <td>53.798241</td>\n",
       "      <td>55.858931</td>\n",
       "      <td>54.126269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Xception</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>22842412</td>\n",
       "      <td>55.290102</td>\n",
       "      <td>53.936037</td>\n",
       "      <td>55.290102</td>\n",
       "      <td>54.131547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ResNet50V2</td>\n",
       "      <td>baseline</td>\n",
       "      <td>23966212</td>\n",
       "      <td>59.215017</td>\n",
       "      <td>59.769347</td>\n",
       "      <td>59.215017</td>\n",
       "      <td>58.142669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ResNet50V2</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>25545732</td>\n",
       "      <td>60.836177</td>\n",
       "      <td>59.936380</td>\n",
       "      <td>60.836177</td>\n",
       "      <td>58.561482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EfficientNetB4</td>\n",
       "      <td>baseline</td>\n",
       "      <td>18025059</td>\n",
       "      <td>58.105802</td>\n",
       "      <td>59.522756</td>\n",
       "      <td>58.105802</td>\n",
       "      <td>58.132060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EfficientNetB4</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>19604579</td>\n",
       "      <td>52.445961</td>\n",
       "      <td>58.853555</td>\n",
       "      <td>52.445961</td>\n",
       "      <td>53.604382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>baseline</td>\n",
       "      <td>7238212</td>\n",
       "      <td>50.142207</td>\n",
       "      <td>48.871944</td>\n",
       "      <td>50.142207</td>\n",
       "      <td>48.073077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>8817732</td>\n",
       "      <td>59.072810</td>\n",
       "      <td>62.179664</td>\n",
       "      <td>59.072810</td>\n",
       "      <td>57.060518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DenseNet169</td>\n",
       "      <td>baseline</td>\n",
       "      <td>12969028</td>\n",
       "      <td>62.599545</td>\n",
       "      <td>67.950688</td>\n",
       "      <td>62.599545</td>\n",
       "      <td>61.555470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DenseNet169</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>14548548</td>\n",
       "      <td>63.054608</td>\n",
       "      <td>64.230583</td>\n",
       "      <td>63.054608</td>\n",
       "      <td>62.026614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>baseline</td>\n",
       "      <td>18698308</td>\n",
       "      <td>51.507395</td>\n",
       "      <td>55.037832</td>\n",
       "      <td>51.507395</td>\n",
       "      <td>51.754089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>20277828</td>\n",
       "      <td>62.428896</td>\n",
       "      <td>63.228191</td>\n",
       "      <td>62.428896</td>\n",
       "      <td>60.697628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  model_type    params   accuracy  precision     recall  \\\n",
       "0         MobileNetV2    baseline   2508868  54.493743  58.765340  54.493743   \n",
       "1         MobileNetV2  multimodal   4088388  54.835040  60.366897  54.835040   \n",
       "2         InceptionV3    baseline  22007588  50.910125  48.080548  50.910125   \n",
       "3         InceptionV3  multimodal  23587108  53.640501  53.134401  53.640501   \n",
       "4   InceptionResNetV2    baseline  54490340  48.577929  47.370282  48.577929   \n",
       "5   InceptionResNetV2  multimodal  56069860  51.393629  50.961373  51.393629   \n",
       "6            Xception    baseline  21262892  55.858931  53.798241  55.858931   \n",
       "7            Xception  multimodal  22842412  55.290102  53.936037  55.290102   \n",
       "8          ResNet50V2    baseline  23966212  59.215017  59.769347  59.215017   \n",
       "9          ResNet50V2  multimodal  25545732  60.836177  59.936380  60.836177   \n",
       "10     EfficientNetB4    baseline  18025059  58.105802  59.522756  58.105802   \n",
       "11     EfficientNetB4  multimodal  19604579  52.445961  58.853555  52.445961   \n",
       "12        DenseNet121    baseline   7238212  50.142207  48.871944  50.142207   \n",
       "13        DenseNet121  multimodal   8817732  59.072810  62.179664  59.072810   \n",
       "14        DenseNet169    baseline  12969028  62.599545  67.950688  62.599545   \n",
       "15        DenseNet169  multimodal  14548548  63.054608  64.230583  63.054608   \n",
       "16        DenseNet201    baseline  18698308  51.507395  55.037832  51.507395   \n",
       "17        DenseNet201  multimodal  20277828  62.428896  63.228191  62.428896   \n",
       "\n",
       "           f1  \n",
       "0   52.715298  \n",
       "1   52.739745  \n",
       "2   49.082164  \n",
       "3   52.586495  \n",
       "4   46.404017  \n",
       "5   49.209224  \n",
       "6   54.126269  \n",
       "7   54.131547  \n",
       "8   58.142669  \n",
       "9   58.561482  \n",
       "10  58.132060  \n",
       "11  53.604382  \n",
       "12  48.073077  \n",
       "13  57.060518  \n",
       "14  61.555470  \n",
       "15  62.026614  \n",
       "16  51.754089  \n",
       "17  60.697628  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
